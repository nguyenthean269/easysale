# Vector Search v·ªõi Milvus - H∆∞·ªõng d·∫´n chi ti·∫øt

## üéØ T·ªïng quan

H·ªá th·ªëng vector search cho ph√©p t√¨m ki·∫øm documents d·ª±a tr√™n semantic similarity thay v√¨ keyword matching. Khi crawl m·ªôt website, n·ªôi dung s·∫Ω ƒë∆∞·ª£c:

1. **Chunking**: Chia th√†nh c√°c ƒëo·∫°n nh·ªè (~300 t·ª´)
2. **Embedding**: Chuy·ªÉn ƒë·ªïi th√†nh vector 768-dimensional
3. **L∆∞u tr·ªØ**: Vector ƒë∆∞·ª£c l∆∞u trong Milvus, metadata trong MySQL
4. **Search**: T√¨m ki·∫øm vector t∆∞∆°ng t·ª± khi user query

## üöÄ C√†i ƒë·∫∑t

### 1. C√†i ƒë·∫∑t Milvus

#### S·ª≠ d·ª•ng Docker (Khuy·∫øn ngh·ªã)
```bash
# T·∫£i Milvus docker-compose
wget https://github.com/milvus-io/milvus/releases/download/v2.3.3/milvus-standalone-docker-compose.yml -O docker-compose.yml

# Kh·ªüi ƒë·ªông Milvus
docker-compose up -d

# Ki·ªÉm tra status
docker-compose ps
```

#### Ho·∫∑c s·ª≠ d·ª•ng pip (cho development)
```bash
pip install pymilvus
```

### 2. C√†i ƒë·∫∑t Dependencies
```bash
cd backend
pip install -r requirements.txt
```

### 3. C·∫•u h√¨nh Environment
Th√™m v√†o file `.env`:
```env
# Milvus Configuration
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION_NAME=document_chunks
MILVUS_DIMENSION=768

# Embedding Model Configuration
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

### 4. Migration Database
```bash
# Th√™m tr∆∞·ªùng milvus_id v√†o b·∫£ng document_chunks
python migrate_link_column.py
```

## üîß Ki·∫øn tr√∫c h·ªá th·ªëng

### 1. VectorService (`utils/vector_service.py`)
- **Kh·ªüi t·∫°o**: Load embedding model v√† k·∫øt n·ªëi Milvus
- **Embedding**: Chuy·ªÉn ƒë·ªïi text th√†nh vector
- **CRUD**: Insert, search, delete vectors trong Milvus
- **Indexing**: T·∫°o index cho vector search

### 2. Database Schema
```sql
-- B·∫£ng document_chunks v·ªõi tr∆∞·ªùng milvus_id
CREATE TABLE document_chunks (
    id INT AUTO_INCREMENT PRIMARY KEY,
    document_id INT NOT NULL,
    chunk_index INT NOT NULL,
    content TEXT NOT NULL,
    milvus_id VARCHAR(100) NULL,  -- ID c·ªßa vector trong Milvus
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
);
```

### 3. Milvus Collection Schema
```python
fields = [
    FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=100),
    FieldSchema(name="document_id", dtype=DataType.INT64),
    FieldSchema(name="chunk_index", dtype=DataType.INT64),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
]
```

## üìù Quy tr√¨nh x·ª≠ l√Ω

### 1. Crawl v√† Vector Processing
```python
# 1. Crawl content t·ª´ website
firecrawl_response = call_firecrawl_api(link)

# 2. Chunking b·∫±ng Groq LLM
chunks = groq_chat.chat(prompt, "H√£y chia chunk")

# 3. L∆∞u v√†o database v√† t·∫°o vector
for chunk in chunks:
    # L∆∞u v√†o MySQL
    document_chunk = DocumentChunk(...)
    db.session.add(document_chunk)
    
    # T·∫°o embedding v√† l∆∞u v√†o Milvus
    milvus_id = vector_service.insert_chunk(
        document_id=document.id,
        chunk_index=chunk_index,
        content=chunk
    )
    
    # C·∫≠p nh·∫≠t milvus_id trong database
    document_chunk.milvus_id = milvus_id
```

### 2. Vector Search
```python
# 1. T·∫°o embedding cho query
query_embedding = vector_service.create_embedding(query)

# 2. Search trong Milvus
search_results = vector_service.search_similar(
    query=query,
    top_k=5,
    document_ids=[1, 2, 3]  # Optional filter
)

# 3. L·∫•y metadata t·ª´ database
for result in search_results:
    chunk = DocumentChunk.query.filter_by(milvus_id=result['id']).first()
    document = Document.query.get(chunk.document_id)
```

## üîç API Endpoints

### 1. Crawl v·ªõi Vector Processing
**POST** `/user/crawls`
```json
{
    "link": "https://example.com",
    "crawl_tool": "firecrawl"
}
```

**Response:**
```json
{
    "message": "Crawl completed successfully",
    "crawl_id": 1,
    "content_length": 1500,
    "chunks_processed": 5
}
```

### 2. Vector Search
**POST** `/user/search`
```json
{
    "query": "machine learning algorithms",
    "top_k": 5,
    "document_ids": [1, 2, 3]  // Optional
}
```

**Response:**
```json
{
    "message": "Search completed successfully",
    "query": "machine learning algorithms",
    "total_results": 3,
    "results": [
        {
            "chunk_id": 1,
            "document_id": 1,
            "document_title": "AI Wikipedia",
            "chunk_index": 2,
            "content": "Machine learning is a subset...",
            "similarity_score": 0.85,
            "source_path": "https://example.com"
        }
    ]
}
```

## üß™ Testing

### 1. Test c∆° b·∫£n
```bash
python test_vector_search.py
```

### 2. Test th·ªß c√¥ng
```bash
# 1. ƒêƒÉng nh·∫≠p
curl -X POST http://localhost:5000/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "user", "password": "user123"}'

# 2. Crawl website
curl -X POST http://localhost:5000/user/crawls \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"link": "https://en.wikipedia.org/wiki/Artificial_intelligence"}'

# 3. Search vector
curl -X POST http://localhost:5000/user/search \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"query": "neural networks", "top_k": 3}'
```

## ‚öôÔ∏è C·∫•u h√¨nh n√¢ng cao

### 1. Embedding Model
C√≥ th·ªÉ thay ƒë·ªïi model trong `.env`:
```env
# C√°c model kh√°c nhau
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2  # 768d, nhanh
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2  # 768d, ch√≠nh x√°c h∆°n
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L12-v2  # 384d, nh·ªè h∆°n
```

### 2. Milvus Index
```python
# IVF_FLAT index (m·∫∑c ƒë·ªãnh)
index_params = {
    "metric_type": "COSINE",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024}
}

# HNSW index (nhanh h∆°n cho search)
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {"M": 16, "efConstruction": 500}
}
```

### 3. Search Parameters
```python
# TƒÉng ƒë·ªô ch√≠nh x√°c
search_params = {
    "metric_type": "COSINE",
    "params": {"nprobe": 20}  # TƒÉng t·ª´ 10 l√™n 20
}

# Gi·∫£m th·ªùi gian search
search_params = {
    "metric_type": "COSINE",
    "params": {"nprobe": 5}  # Gi·∫£m t·ª´ 10 xu·ªëng 5
}
```

## üîß Troubleshooting

### 1. Milvus Connection Error
```bash
# Ki·ªÉm tra Milvus status
docker-compose ps

# Restart Milvus
docker-compose restart

# Ki·ªÉm tra logs
docker-compose logs milvus-standalone
```

### 2. Embedding Model Error
```bash
# X√≥a cache model
rm -rf ~/.cache/torch/sentence_transformers/

# Reinstall sentence-transformers
pip uninstall sentence-transformers
pip install sentence-transformers
```

### 3. Memory Issues
```python
# Gi·∫£m batch size khi insert
vector_service.insert_chunk_batch(chunks, batch_size=10)

# S·ª≠ d·ª•ng model nh·ªè h∆°n
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

## üìä Performance Optimization

### 1. Batch Processing
```python
def insert_chunks_batch(chunks_data):
    """Insert nhi·ªÅu chunks c√πng l√∫c"""
    embeddings = []
    ids = []
    document_ids = []
    chunk_indices = []
    
    for chunk_data in chunks_data:
        embedding = vector_service.create_embedding(chunk_data['content'])
        embeddings.append(embedding)
        ids.append(str(uuid.uuid4()))
        document_ids.append(chunk_data['document_id'])
        chunk_indices.append(chunk_data['chunk_index'])
    
    # Insert batch
    data = [ids, document_ids, chunk_indices, embeddings]
    vector_service.collection.insert(data)
```

### 2. Caching
```python
# Cache embedding model
@lru_cache(maxsize=1)
def get_embedding_model():
    return SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Cache search results
@lru_cache(maxsize=100)
def cached_search(query, top_k=5):
    return vector_service.search_similar(query, top_k)
```

### 3. Async Processing
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def process_chunks_async(chunks):
    """X·ª≠ l√Ω chunks b·∫•t ƒë·ªìng b·ªô"""
    with ThreadPoolExecutor() as executor:
        tasks = []
        for chunk in chunks:
            task = asyncio.create_task(
                asyncio.get_event_loop().run_in_executor(
                    executor, 
                    vector_service.insert_chunk,
                    chunk['document_id'],
                    chunk['chunk_index'],
                    chunk['content']
                )
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results
```

## üéØ Best Practices

### 1. Chunking Strategy
- **K√≠ch th∆∞·ªõc**: 200-400 t·ª´ m·ªói chunk
- **Overlap**: 50-100 t·ª´ gi·ªØa c√°c chunks
- **Semantic boundaries**: Chia theo c√¢u/ƒëo·∫°n vƒÉn

### 2. Embedding Quality
- **Preprocessing**: Lo·∫°i b·ªè HTML, normalize text
- **Model selection**: Ch·ªçn model ph√π h·ª£p v·ªõi ng√¥n ng·ªØ
- **Fine-tuning**: Fine-tune model cho domain c·ª• th·ªÉ

### 3. Search Optimization
- **Query preprocessing**: Normalize query text
- **Filtering**: S·ª≠ d·ª•ng document_ids ƒë·ªÉ filter
- **Scoring**: K·∫øt h·ª£p similarity score v·ªõi metadata

### 4. Monitoring
```python
# Log performance metrics
import time

start_time = time.time()
results = vector_service.search_similar(query, top_k)
search_time = time.time() - start_time

logger.info(f"Search completed in {search_time:.2f}s for query: {query}")
```

## üîÆ Roadmap

### Phase 1: Basic Vector Search ‚úÖ
- [x] Milvus integration
- [x] Embedding generation
- [x] Basic search API

### Phase 2: Advanced Features
- [ ] Hybrid search (vector + keyword)
- [ ] Semantic clustering
- [ ] Auto-suggestions
- [ ] Search analytics

### Phase 3: Production Ready
- [ ] Load balancing
- [ ] Caching layer
- [ ] Monitoring dashboard
- [ ] A/B testing framework 