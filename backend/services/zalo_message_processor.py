"""
Zalo Message Processor Service
Service x·ª≠ l√Ω tin nh·∫Øn t·ª´ Zalo ƒë·ªãnh k·ª≥ 10 ph√∫t m·ªôt l·∫ßn
"""

import os
import time
import threading
import logging
import json
import argparse
from datetime import datetime
from typing import List, Dict, Optional
from groq import Groq
from dotenv import load_dotenv
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from utils.property_service_sql import PropertyService
from .warehouse_database_service import warehouse_service

# Load environment variables
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', '.env'))

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Debug environment variables
logger.info("üîß Environment variables loaded:")
logger.info(f"DB_CHAT_HOST: {os.getenv('DB_CHAT_HOST', 'NOT_SET')}")
logger.info(f"DB_CHAT_PORT: {os.getenv('DB_CHAT_PORT', 'NOT_SET')}")
logger.info(f"DB_CHAT_USER: {os.getenv('DB_CHAT_USER', 'NOT_SET')}")
logger.info(f"DB_CHAT_PASSWORD: {'SET' if os.getenv('DB_CHAT_PASSWORD') else 'NOT_SET'}")
logger.info(f"DB_NAME: {os.getenv('DB_NAME', 'NOT_SET')}")
logger.info(f"DB_WAREHOUSE_HOST: {os.getenv('DB_WAREHOUSE_HOST', 'NOT_SET')}")
logger.info(f"DB_WAREHOUSE_PORT: {os.getenv('DB_WAREHOUSE_PORT', 'NOT_SET')}")
logger.info(f"DB_WAREHOUSE_USER: {os.getenv('DB_WAREHOUSE_USER', 'NOT_SET')}")
logger.info(f"DB_WAREHOUSE_PASSWORD: {'SET' if os.getenv('DB_WAREHOUSE_PASSWORD') else 'NOT_SET'}")
logger.info(f"DB_WAREHOUSE_NAME: {os.getenv('DB_WAREHOUSE_NAME', 'NOT_SET')}")

# T·∫°o Flask app cho easychat database
zalo_app = Flask(__name__)
zalo_app.config['SQLALCHEMY_DATABASE_URI'] = f"mysql://{os.getenv('DB_CHAT_USER', 'easychat')}:{os.getenv('DB_CHAT_PASSWORD', '')}@{os.getenv('DB_CHAT_HOST', '103.6.234.59')}:{os.getenv('DB_CHAT_PORT', '6033')}/{os.getenv('DB_NAME', 'easychat')}"
zalo_app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
zalo_db = SQLAlchemy(zalo_app)

# Warehouse database service ƒë∆∞·ª£c import t·ª´ warehouse_database_service.py

class ZaloMessageProcessor:
    """Service x·ª≠ l√Ω tin nh·∫Øn Zalo ƒë·ªãnh k·ª≥"""
    
    def __init__(self):
        """Kh·ªüi t·∫°o service"""
        # Groq client
        self.groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))
        
        # Service control
        self.is_running = False
        self.thread = None
        
        # L·∫•y interval t·ª´ environment variable (ƒë∆°n v·ªã ph√∫t)
        # ZALO_MESSAGE_PROCESSOR_SCHEDULE=0 nghƒ©a l√† t·∫Øt schedule m·∫∑c ƒë·ªãnh
        # Nh∆∞ng v·∫´n c√≥ th·ªÉ start t·ª´ UI
        schedule_minutes = int(os.getenv('ZALO_MESSAGE_PROCESSOR_SCHEDULE', '10'))
        self.default_interval = schedule_minutes * 60  # Chuy·ªÉn t·ª´ ph√∫t sang gi√¢y (gi√° tr·ªã m·∫∑c ƒë·ªãnh)
        self.interval = self.default_interval  # Interval hi·ªán t·∫°i (c√≥ th·ªÉ thay ƒë·ªïi t·ª´ UI)
        self.schedule_enabled = schedule_minutes > 0  # Flag m·∫∑c ƒë·ªãnh: 0 = t·∫Øt, >0 = b·∫≠t
        self.started_at = None  # Th·ªùi gian b·∫Øt ƒë·∫ßu schedule
        
        # Warehouse service instance
        self.warehouse_service = warehouse_service
        
        logger.info(f"ZaloMessageProcessor initialized (schedule: {self.interval//60} minutes, enabled: {self.schedule_enabled})")
    
    def get_zalo_db_connection(self):
        """T·∫°o k·∫øt n·ªëi database easychat v·ªõi retry mechanism"""
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Attempting to connect to easychat database... (attempt {attempt + 1}/{max_retries})")
                
                with zalo_app.app_context():
                    # S·ª≠ d·ª•ng SQLAlchemy engine v·ªõi connection pooling
                    connection = zalo_db.engine.connect()
                    logger.info("‚úÖ Easychat database connection successful")
                    return connection
                    
            except Exception as e:
                logger.error(f"‚ùå Easychat database connection error (attempt {attempt + 1}): {e}")
                logger.error(f"Error type: {type(e)}")
                
                if attempt < max_retries - 1:
                    logger.info(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    logger.error("‚ùå Failed to connect to easychat database after all retries")
        
        return None
    
    def get_warehouse_db_connection(self):
        """T·∫°o k·∫øt n·ªëi database warehouse v·ªõi retry mechanism"""
        return self.warehouse_service.get_warehouse_db_connection()
    
    def get_unprocessed_messages(self, limit: int = 20, offset: int = 0, warehouse_id: str = 'NULL') -> List[Dict]:
        """
        L·∫•y danh s√°ch tin nh·∫Øn t·ª´ b·∫£ng zalo_received_messages trong database easychat theo warehouse_id
        
        Args:
            limit: S·ªë l∆∞·ª£ng tin nh·∫Øn t·ªëi ƒëa c·∫ßn l·∫•y
            offset: S·ªë l∆∞·ª£ng tin nh·∫Øn b·ªè qua (cho pagination)
            warehouse_id: Tr·∫°ng th√°i warehouse_id ('NULL', 'NOT_NULL', 'ALL')
            
        Returns:
            List c√°c tin nh·∫Øn theo warehouse_id
        """
        try:
            logger.info("üîç Starting to fetch unprocessed messages...")
            
            # Retry logic cho connection
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    logger.info(f"üîÑ Attempt {attempt + 1}/{max_retries}")
                    
                    with zalo_app.app_context():
                        logger.info("üì° Zalo app context created")
                        connection = self.get_zalo_db_connection()
                        if not connection:
                            logger.error("‚ùå No database connection available")
                            continue
                        
                        logger.info("üìä Executing query to fetch messages...")
                        from sqlalchemy import text
                        
                        # X√¢y d·ª±ng WHERE clause d·ª±a tr√™n warehouse_id
                        # Lu√¥n th√™m ƒëi·ªÅu ki·ªán content_hash IS NOT NULL ƒë·ªÉ ch·ªâ l·∫•y messages unique
                        if warehouse_id == 'ALL':
                            where_clause = "WHERE content_hash IS NOT NULL"
                            params = {"limit": limit, "offset": offset}
                        elif warehouse_id == 'NULL':
                            where_clause = "WHERE warehouse_id IS NULL AND content_hash IS NOT NULL"
                            params = {"limit": limit, "offset": offset}
                        elif warehouse_id == 'NOT_NULL':
                            where_clause = "WHERE warehouse_id IS NOT NULL AND content_hash IS NOT NULL"
                            params = {"limit": limit, "offset": offset}
                        else:
                            # N·∫øu warehouse_id l√† m·ªôt s·ªë c·ª• th·ªÉ
                            where_clause = "WHERE warehouse_id = :warehouse_id AND content_hash IS NOT NULL"
                            params = {"limit": limit, "offset": offset, "warehouse_id": warehouse_id}
                        
                        # Query ƒë·ªÉ ƒë·∫øm t·ªïng s·ªë records (unique content_hash)
                        # Ch·ªâ ƒë·∫øm c√°c content_hash unique, kh√¥ng tr√πng l·∫∑p
                        count_query = text(f"""
                        SELECT COUNT(DISTINCT content_hash) as total
                        FROM zalo_received_messages 
                        {where_clause}
                        """)
                        
                        # Query ƒë·ªÉ l·∫•y data v·ªõi pagination - ch·ªâ l·∫•y messages unique theo content_hash
                        # S·ª≠ d·ª•ng subquery ƒë·ªÉ l·∫•y MIN(id) cho m·ªói content_hash unique
                        data_query = text(f"""
                        SELECT z.id, z.session_id, z.config_id, z.sender_id, z.sender_name, 
                               z.content, z.thread_id, z.thread_type, z.received_at, 
                               z.status_push_kafka, z.warehouse_id, z.reply_quote,
                               z.content_hash, z.added_document_chunks
                        FROM zalo_received_messages z
                        INNER JOIN (
                            SELECT content_hash, MIN(id) as min_id
                            FROM zalo_received_messages
                            {where_clause}
                            GROUP BY content_hash
                            ORDER BY MIN(received_at) ASC
                            LIMIT :limit OFFSET :offset
                        ) unique_hashes ON z.content_hash = unique_hashes.content_hash AND z.id = unique_hashes.min_id
                        ORDER BY z.received_at ASC
                        """)
                        
                        logger.info(f"üîç Data Query: {data_query}")
                        logger.info(f"üîç Limit: {limit}, Offset: {offset}")
                        logger.info(f"üîç Warehouse ID filter: {warehouse_id}")
                        
                        # ƒê·∫øm t·ªïng s·ªë records (unique content_hash)
                        count_params = {k: v for k, v in params.items() if k != 'limit' and k != 'offset'}
                        count_result = connection.execute(count_query, count_params)
                        total_count = count_result.fetchone()[0]
                        
                        # L·∫•y data
                        result = connection.execute(data_query, params)
                        logger.info("‚úÖ Query executed successfully")
                        
                        messages = []
                        for row in result:
                            messages.append(dict(row._mapping))
                        
                        logger.info(f"‚úÖ Found {len(messages)} unprocessed messages out of {total_count} total")
                        # Return messages with total count as metadata
                        # We'll modify the return to include total in a dict
                        return {'messages': messages, 'total': total_count}
                        
                except Exception as e:
                    logger.error(f"‚ùå Attempt {attempt + 1} failed: {e}")
                    if attempt < max_retries - 1:
                        logger.info(f"üîÑ Retrying in 2 seconds...")
                        time.sleep(2)
                    else:
                        raise e
                        
        except Exception as e:
            logger.error(f"‚ùå Error fetching messages after {max_retries} attempts: {e}")
            logger.error(f"‚ùå Error type: {type(e)}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return {'messages': [], 'total': 0}
    
    def get_property_tree_for_prompt(self, root_id: int = 1) -> str:
        """
        L·∫•y property tree cho prompt
        
        Args:
            root_id (int): ID c·ªßa root group (m·∫∑c ƒë·ªãnh l√† 1)
            
        Returns:
            str: Property tree ƒë√£ format cho prompt
        """
        return self.warehouse_service.get_property_tree_for_prompt(root_id)
    
    def process_message_with_groq(self, message_content: str) -> Optional[str]:
        """
        G·ª≠i tin nh·∫Øn t·ªõi Groq API ƒë·ªÉ b√≥c t√°ch th√¥ng tin cƒÉn h·ªô
        
        Args:
            message_content: N·ªôi dung tin nh·∫Øn c·∫ßn x·ª≠ l√Ω
            
        Returns:
            K·∫øt qu·∫£ t·ª´ Groq API ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # L·∫•y property tree t·ª´ database
            property_tree = self.get_property_tree_for_prompt()
            
            # Prompt ƒë·ªÉ Groq tr·∫£ v·ªÅ JSON
            prompt = f"""
            H√£y ph√¢n t√≠ch tin nh·∫Øn rao b√°n cƒÉn h·ªô trong c·∫∑p th·∫ª XML <message></message> v√† tr·∫£ v·ªÅ  duy nh·∫•t JSON string ch·ª©a th√¥ng tin cƒÉn h·ªô nh∆∞ ƒë∆∞·ª£c m√¥ t·∫£ trong c·∫∑p th·∫ª XML <output></output>
            
            <message>
            {message_content}
            </message>
            
            

            <output>
{{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Danh s√°ch cƒÉn h·ªô rao b√°n",
  "type": "array",
  "items": {{
    "type": "object",
    "title": "Th√¥ng tin cƒÉn h·ªô rao b√°n",
    "properties": {{
      "message_id": {{
        "type": "number",
        "description": "ID ƒë·ªÉ ph·ª•c v·ª• tracking"
      }},
      "property_group": {{
        "type": "number",
        "description": "Ng∆∞·ªùi d√πng ƒëang c√≥ cƒÉn h·ªô rao b√°n t·∫°i t√≤a c√≥ ID l√† bao nhi√™u? S·ª≠ d·ª•ng th√¥ng tin trong c·∫∑p th·∫ª XML <thong-tin-du-an></thong-tin-du-an> ƒë·ªÉ x√°c ƒë·ªãnh."
      }},
      "unit_code": {{
        "type": ["string", "null"],
        "description": "M√£ cƒÉn h·ªô n·∫øu c√≥. M√£ cƒÉn h·ªô th∆∞·ªùng ƒë∆∞·ª£c gh√©p t·ª´ s·ªë t·∫ßng v√† s·ªë tr·ª•c cƒÉn. V√≠ d·ª•: 0812, 08A15A"
      }},
      "unit_axis": {{
        "type": ["string", "null"],
        "description": "Tr·ª•c cƒÉn n·∫øu c√≥. V√≠ d·ª•: 12, 15A"
      }},
      "unit_floor_number": {{
        "type": ["integer", "null"],
        "description": "T·∫ßng n·∫øu c√≥. V√≠ d·ª•: 08, 08A"
      }},
      "area_land": {{
        "type": ["number", "null"],
        "description": "Di·ªán t√≠ch ƒë·∫•t n·∫øu c√≥"
      }},
      "area_construction": {{
        "type": ["number", "null"],
        "description": "Di·ªán t√≠ch x√¢y d·ª±ng n·∫øu c√≥"
      }},
      "area_net": {{
        "type": ["number", "null"],
        "description": "Di·ªán t√≠ch th√¥ng th·ªßy n·∫øu c√≥"
      }},
      "area_gross": {{
        "type": ["number", "null"],
        "description": "Di·ªán t√≠ch tim t∆∞·ªùng n·∫øu c√≥"
      }},
      "num_bedrooms": {{
        "type": ["integer", "null"],
        "description": "S·ªë ph√≤ng ng·ªß n·∫øu c√≥"
      }},
      "num_bathrooms": {{
        "type": ["integer", "null"],
        "description": "S·ªë ph√≤ng t·∫Øm n·∫øu c√≥"
      }},
      "unit_type": {{
        "type": "number",
        "description": "ID c·ªßa lo·∫°i cƒÉn h·ªô"
      }},
      "direction_door": {{
        "type": ["string", "null"],
        "enum": ["D", "T", "N", "B", "DB", "DN", "TB", "TN", null],
        "description": "H∆∞·ªõng c·ª≠a ch√≠nh"
      }},
      "direction_balcony": {{
        "type": ["string", "null"],
        "enum": ["D", "T", "N", "B", "DB", "DN", "TB", "TN", null],
        "description": "H∆∞·ªõng ban c√¥ng"
      }},
      "price": {{
        "type": "number",
        "description": "Gi√° n·∫øu c√≥. ƒê∆°n v·ªã VNƒê"
      }},
      "price_early": {{
        "type": "number",
        "description": "Gi√° thanh to√°n s·ªõm n·∫øu c√≥. ƒê∆°n v·ªã VNƒê"
      }},
      "price_schedule": {{
        "type": "number",
        "description": "Gi√° thanh to√°n theo ti·∫øn ƒë·ªô n·∫øu c√≥. ƒê∆°n v·ªã VNƒê"
      }},
      "price_loan": {{
        "type": "number",
        "description": "Gi√° vay ng√¢n h√†ng n·∫øu c√≥. ƒê∆°n v·ªã VNƒê"
      }},
      "price_rent": {{
        "type": "number",
        "description": "Gi√° cho thu√™ n·∫øu c√≥. ƒê∆°n v·ªã VNƒê"
      }},
      "phone_number": {{
        "type": ["string", "null"],
        "description": "S·ªë ƒëi·ªán tho·∫°i li√™n h·ªá"
      }},
      "listing_type": {{
        "type": "string",
        "enum": ["CAN_THUE", "CAN_CHO_THUE", "CAN_BAN", "CAN_MUA", "KHAC"],
        "description": "M·ª•c ƒë√≠ch tin ƒëƒÉng: c·∫ßn thu√™, c·∫ßn cho thu√™, c·∫ßn b√°n, c·∫ßn mua, kh√°c"
      }},
      "notes": {{
        "type": ["string", "null"],
        "description": "Ghi ch√∫ n·∫øu c√≥"
      }},
      "status": {{
        "type": ["string", "null"],
        "enum": ["CHUA_BAN", "DA_LOCK", "DA_COC", "DA_BAN", null],
        "description": "Tr·∫°ng th√°i n·∫øu c√≥"
      }},
      "furnished_status": {{
        "type": ["string", "null"],
        "enum": ["FULL", "PARTIAL", "UNFURNISHED", null],
        "description": "T√¨nh tr·∫°ng n·ªôi th·∫•t: FULL (ƒë·∫ßy ƒë·ªß n·ªôi th·∫•t), PARTIAL (n·ªôi th·∫•t m·ªôt ph·∫ßn), UNFURNISHED (kh√¥ng n·ªôi th·∫•t)"
      }},
      "floor_level_category": {{
        "type": ["string", "null"],
        "enum": ["LOW", "MEDIUM", "HIGH", null],
        "description": "V·ªã tr√≠ t·∫ßng: LOW (t·∫ßng th·∫•p 1-10), MEDIUM (t·∫ßng trung 11-25), HIGH (t·∫ßng cao >25)"
      }},
      "move_in_ready": {{
        "type": ["boolean", "null"],
        "description": "CƒÉn h·ªô c√≥ s·∫µn s√†ng ƒë·ªÉ v√†o ·ªü ngay kh√¥ng: true=s·∫µn s√†ng, false=ch∆∞a s·∫µn s√†ng"
      }},
      "includes_transfer_fees": {{
        "type": ["boolean", "null"],
        "description": "Gi√° ƒë√£ bao g·ªìm c√°c lo·∫°i ph√≠ chuy·ªÉn nh∆∞·ª£ng hay ch∆∞a: true=ƒë√£ bao g·ªìm, false=ch∆∞a bao g·ªìm"
      }}
    }},
    "required": ["property_group"],
    "additionalProperties": false
  }}
}}

</output>



            {property_tree}

            L∆∞u √Ω quan tr·ªçng:
            - Ch·ªâ tr·∫£ v·ªÅ duy nh·∫•t JSON, kh√¥ng c√≥ n·ªôi dung n√†o kh√°c.
            - Kh√¥ng di·ªÖn gi·∫£i, ch·ªâ tr·∫£ v·ªÅ JSON.
            - N·∫øu ng∆∞·ªùi d√πng ƒë·ªÅ c·∫≠p di·ªán t√≠ch m√† kh√¥ng n√≥i l√† lo·∫°i di·ªán t√≠ch g√¨ th√¨ ƒë√≥ ch√≠nh l√† di·ªán t√≠ch tim t∆∞·ªùng.
            - N·∫øu ng∆∞·ªùi d√πng ƒë·ªÅ c·∫≠p h∆∞·ªõng m√† kh√¥ng n√≥i l√† h∆∞·ªõng c·ª≠a ch√≠nh hay h∆∞·ªõng ban c√¥ng th√¨ ƒë√≥ ch√≠nh l√† h∆∞·ªõng c·ª≠a ch√≠nh.
            - N·∫øu b√†i ƒëƒÉng c√≥ gi√° ti·ªÅn tri·ªáu th√¨ ƒë√≥ l√† gi√° thu√™, gi√° ti·ªÅn t·ª∑ th√¨ ƒë√≥ l√† gi√° b√°n.
            - N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin n√†o, tr·∫£ v·ªÅ null cho tr∆∞·ªùng ƒë√≥.
            - N·∫øu b√†i ƒëƒÉng ghi t·∫ßng 1x th√¨ ƒë√≥ l√† kho·∫£ng t·∫ßng 11 ƒë·∫øn 19
            - Vi·∫øt "TC 7tr5" nghƒ©a l√† t√†i ch√≠nh 7 tri·ªáu 500 ng√†n , √Ω l√† t√†i ch√≠nh (ng√¢n s√°ch) 7.5 tri·ªáu
            
            L∆∞u √Ω v·ªÅ c√°c tr∆∞·ªùng m·ªõi:
            - furnished_status: T√¨m t·ª´ kh√≥a "ƒë·∫ßy ƒë·ªß n·ªôi th·∫•t", "full n·ªôi th·∫•t", "c√≥ n·ªôi th·∫•t", "n·ªôi th·∫•t cao c·∫•p" => FULL; "m·ªôt ph·∫ßn n·ªôi th·∫•t", "n·ªôi th·∫•t c∆° b·∫£n" => PARTIAL; "kh√¥ng n·ªôi th·∫•t", "th√¥", "b√†n giao th√¥" => UNFURNISHED
            - floor_level_category: D·ª±a v√†o unit_floor_number ho·∫∑c m√¥ t·∫£ trong tin nh·∫Øn. T·∫ßng 1-10 => LOW, t·∫ßng 11-25 => MEDIUM, t·∫ßng >25 => HIGH. N·∫øu ch·ªâ n√≥i "view ƒë·∫πp", "t·∫ßng cao", "view tho√°ng" m√† kh√¥ng n√≥i s·ªë t·∫ßng c·ª• th·ªÉ th√¨ ch·ªçn HIGH
            - move_in_ready: T√¨m t·ª´ kh√≥a "v√†o ·ªü ngay", "s·∫µn s√†ng", "b√†n giao ngay", "ƒëang tr·ªëng", "c√≥ th·ªÉ chuy·ªÉn v√†o ngay" => true; "ƒëang cho thu√™", "c·∫ßn s·ª≠a sang", "ƒëang ·ªü" => false
            - includes_transfer_fees: T√¨m t·ª´ kh√≥a "gi√° full ph√≠", "bao g·ªìm ph√≠", "ƒë√£ bao g·ªìm ph√≠ chuy·ªÉn nh∆∞·ª£ng", "gi√° net" => true; "ch∆∞a g·ªìm ph√≠", "ph√≠ chuy·ªÉn nh∆∞·ª£ng ri√™ng", "gi√° ch∆∞a VAT" => false

            
            """

            print("prompt")
            print(prompt)
            # exit()
            
            completion = self.groq_client.chat.completions.create(
                model="openai/gpt-oss-120b",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,  # Gi·∫£m temperature ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n v·ªõi GPT-OSS
                max_completion_tokens=36751,  # TƒÉng token limit cho GPT-OSS
                top_p=0.9,  # Gi·∫£m top_p ƒë·ªÉ t·∫≠p trung h∆°n
                stream=False,
                stop=None
            )
            
            # Collect streaming response v·ªõi error handling t·ªët h∆°n
            response_content = completion.choices[0].message.content
            # print('response_content2', completion.choices[0].message)
            # try:
            #     for chunk in completion:
            #         if hasattr(chunk, 'choices') and chunk.choices and len(chunk.choices) > 0:
            #             if hasattr(chunk.choices[0], 'delta') and chunk.choices[0].delta:
            #                 if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
            #                     response_content += chunk.choices[0].delta.content
            #     print('response_content', response_content)
            # except Exception as stream_error:
            #     logger.error(f"Error in streaming response: {stream_error}")
            #     # Fallback: th·ª≠ l·∫•y response kh√¥ng streaming
            #     try:
            #         completion_no_stream = self.groq_client.chat.completions.create(
            #             model="openai/gpt-oss-120b",
            #             messages=[
            #                 {
            #                     "role": "user",
            #                     "content": prompt
            #                 }
            #             ],
            #             temperature=0.1,
            #             max_completion_tokens=2048,
            #             top_p=0.9,
            #             stream=False
            #         )
            #         response_content = completion_no_stream.choices[0].message.content
            #     except Exception as fallback_error:
            #         logger.error(f"Fallback also failed: {fallback_error}")
            #         return None
            
            logger.info(f"GPT-OSS processing completed for message, response length: {len(response_content)}")
            return response_content
            
        except Exception as e:
            logger.error(f"Error processing message with Groq: {e}")
            return None
    
    def parse_groq_response(self, groq_response: str) -> Optional[Dict]:
        """
        Parse JSON response t·ª´ Groq/GPT-OSS
        
        Args:
            groq_response: Response t·ª´ Groq API
            
        Returns:
            Dict ch·ª©a th√¥ng tin cƒÉn h·ªô ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # Clean response ƒë·ªÉ l·∫•y JSON
            response_clean = groq_response.strip()
            
            # Log raw response ƒë·ªÉ debug
            logger.info(f"Raw GPT-OSS response: {response_clean[:300]}...")
            
            # T√¨m JSON trong response (c√≥ th·ªÉ c√≥ text kh√°c)
            start_idx = response_clean.find('{')
            end_idx = response_clean.rfind('}')
            
            if start_idx == -1 or end_idx == -1:
                logger.error("No JSON found in GPT-OSS response")
                logger.error(f"Full response: {response_clean}")
                return None
            
            json_str = response_clean[start_idx:end_idx + 1]
            logger.info(f"Extracted JSON: {json_str}")
            
            apartment_data = json.loads(json_str)
            
            # Validate v√† fix data types cho GPT-OSS
            apartment_data = PropertyService.validate_and_fix_apartment_data(apartment_data)
            
            logger.info(f"Successfully parsed GPT-OSS response: {len(apartment_data)} fields")
            return apartment_data
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            logger.error(f"Raw response: {groq_response[:500]}...")
            
            # Th·ª≠ parse l·∫°i v·ªõi m·ªôt s·ªë fix c∆° b·∫£n cho GPT-OSS
            try:
                # Fix common JSON issues v·ªõi GPT-OSS
                fixed_response = response_clean
                
                # Fix c√°c l·ªói ph·ªï bi·∫øn
                fixes = [
                    ('"property_group": "S401"', '"property_group": 401'),
                    ('"unit_axis', '"unit_axis"'),
                    ('"unit_code', '"unit_code"'),
                    ('"price": "3.2 t·ª∑"', '"price": 3200000000'),
                    ('"area_gross": "85m2"', '"area_gross": 85'),
                    ('"num_bedrooms": "2PN"', '"num_bedrooms": 2'),
                    ('"num_bathrooms": "2WC"', '"num_bathrooms": 2'),
                ]
                
                for old, new in fixes:
                    fixed_response = fixed_response.replace(old, new)
                
                # T√¨m JSON l·∫°i
                start_idx = fixed_response.find('{')
                end_idx = fixed_response.rfind('}')
                
                if start_idx != -1 and end_idx != -1:
                    json_str = fixed_response[start_idx:end_idx + 1]
                    apartment_data = json.loads(json_str)
                    apartment_data = PropertyService.validate_and_fix_apartment_data(apartment_data)
                    logger.info("Successfully parsed after fixing JSON")
                    return apartment_data
                    
            except Exception as fix_error:
                logger.error(f"Failed to fix JSON: {fix_error}")
            
            return None
        except Exception as e:
            logger.error(f"Error parsing GPT-OSS response: {e}")
            return None
    
    def map_unit_type_to_id(self, unit_type_name) -> Optional[int]:
        """
        Map unit type name sang ID
        
        Args:
            unit_type_name: T√™n lo·∫°i cƒÉn h·ªô (c√≥ th·ªÉ l√† string ho·∫∑c int)
            
        Returns:
            ID t∆∞∆°ng ·ª©ng ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        return self.warehouse_service.map_unit_type_to_id(unit_type_name)
    
    def insert_apartment_via_api(self, apartment_data: Dict) -> bool:
        """
        Insert apartment v√†o warehouse database th√¥ng qua API
        
        Args:
            apartment_data: D·ªØ li·ªáu cƒÉn h·ªô t·ª´ Groq
            
        Returns:
            True n·∫øu th√†nh c√¥ng, False n·∫øu l·ªói
        """
        return self.warehouse_service.insert_apartment_via_api(apartment_data)
    
    def update_message_warehouse_id(self, message_id: int, warehouse_id: int) -> bool:
        """
        C·∫≠p nh·∫≠t warehouse_id c·ªßa tin nh·∫Øn sau khi x·ª≠ l√Ω v·ªõi retry mechanism
        DEPRECATED: S·ª≠ d·ª•ng update_warehouse_id_by_content_hash() ƒë·ªÉ update t·∫•t c·∫£ messages c√πng content_hash
        
        Args:
            message_id: ID c·ªßa tin nh·∫Øn
            warehouse_id: ID c·ªßa apartment trong warehouse database
            
        Returns:
            True n·∫øu c·∫≠p nh·∫≠t th√†nh c√¥ng, False n·∫øu l·ªói
        """
        # L·∫•y content_hash t·ª´ message ƒë·ªÉ update t·∫•t c·∫£ messages c√πng content_hash
        message = self.get_message_by_id(message_id)
        if message and message.get('content_hash'):
            return self.update_warehouse_id_by_content_hash(message['content_hash'], warehouse_id)
        else:
            # Fallback: update ch·ªâ message n√†y n·∫øu kh√¥ng c√≥ content_hash
            return self._update_single_message_warehouse_id(message_id, warehouse_id)
    
    def update_warehouse_id_by_content_hash(self, content_hash: str, warehouse_id: int) -> bool:
        """
        C·∫≠p nh·∫≠t warehouse_id cho T·∫§T C·∫¢ c√°c messages c√≥ c√πng content_hash
        
        Args:
            content_hash: Hash c·ªßa n·ªôi dung message
            warehouse_id: ID c·ªßa apartment trong warehouse database
            
        Returns:
            True n·∫øu c·∫≠p nh·∫≠t th√†nh c√¥ng, False n·∫øu l·ªói
        """
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            connection = None
            try:
                logger.info(f"Updating warehouse_id to {warehouse_id} for all messages with content_hash={content_hash} (attempt {attempt + 1}/{max_retries})")
                
                with zalo_app.app_context():
                    connection = self.get_zalo_db_connection()
                    if not connection:
                        logger.error(f"No database connection available (attempt {attempt + 1})")
                        continue
                
                from sqlalchemy import text
                query = text("""
                UPDATE zalo_received_messages 
                SET warehouse_id = :warehouse_id 
                WHERE content_hash = :content_hash
                """)
                
                result = connection.execute(query, {"warehouse_id": warehouse_id, "content_hash": content_hash})
                connection.commit()
                
                # Check if any rows were affected
                rows_affected = result.rowcount
                if rows_affected > 0:
                    logger.info(f"‚úÖ Updated {rows_affected} message(s) with content_hash={content_hash} to warehouse_id={warehouse_id}")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è No rows affected when updating messages with content_hash={content_hash}")
                    return False
                
            except Exception as e:
                logger.error(f"‚ùå Error updating warehouse_id by content_hash (attempt {attempt + 1}): {e}")
                logger.error(f"Error type: {type(e)}")
                
                if attempt < max_retries - 1:
                    logger.info(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    logger.error("‚ùå Failed to update warehouse_id by content_hash after all retries")
                return False
                    
            finally:
                if connection:
                    try:
                        connection.close()
                    except Exception as close_error:
                        logger.warning(f"Warning: Error closing connection: {close_error}")
        
        return False
    
    def _update_single_message_warehouse_id(self, message_id: int, warehouse_id: int) -> bool:
        """
        C·∫≠p nh·∫≠t warehouse_id c·ªßa m·ªôt tin nh·∫Øn c·ª• th·ªÉ (fallback method)
        
        Args:
            message_id: ID c·ªßa tin nh·∫Øn
            warehouse_id: ID c·ªßa apartment trong warehouse database
            
        Returns:
            True n·∫øu c·∫≠p nh·∫≠t th√†nh c√¥ng, False n·∫øu l·ªói
        """
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            connection = None
            try:
                logger.info(f"Updating message {message_id} warehouse_id to {warehouse_id} (attempt {attempt + 1}/{max_retries})")
                
                with zalo_app.app_context():
                    connection = self.get_zalo_db_connection()
                    if not connection:
                        logger.error(f"No database connection available (attempt {attempt + 1})")
                        continue
                
                from sqlalchemy import text
                query = text("""
                UPDATE zalo_received_messages 
                SET warehouse_id = :warehouse_id 
                WHERE id = :message_id
                """)
                
                result = connection.execute(query, {"warehouse_id": warehouse_id, "message_id": message_id})
                connection.commit()
                
                # Check if any rows were affected
                rows_affected = result.rowcount
                if rows_affected > 0:
                    logger.info(f"‚úÖ Updated message {message_id} warehouse_id to {warehouse_id} (rows affected: {rows_affected})")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è No rows affected when updating message {message_id} warehouse_id to {warehouse_id}")
                    return False
                
            except Exception as e:
                logger.error(f"‚ùå Error updating message warehouse_id (attempt {attempt + 1}): {e}")
                logger.error(f"Error type: {type(e)}")
                
                if attempt < max_retries - 1:
                    logger.info(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    logger.error("‚ùå Failed to update message warehouse_id after all retries")
                return False
                    
            finally:
                if connection:
                    try:
                        connection.close()
                    except Exception as close_error:
                        logger.warning(f"Warning: Error closing connection: {close_error}")
        
        return False
    
    def process_messages_batch(self, limit: int = 20):
        """
        X·ª≠ l√Ω m·ªôt batch tin nh·∫Øn (s·ª≠ d·ª•ng batch processing gi·ªëng nh∆∞ /api/zalo-test/process-message)
        - G·ªôp nhi·ªÅu messages v√†o 1 prompt, g·ª≠i 1 request Groq
        - Set data_status='REVIEWING' cho t·∫•t c·∫£ apartments
        - C·∫≠p nh·∫≠t warehouse_id cho t·∫•t c·∫£ messages c√πng content_hash
        """
        logger.info(f"Starting message batch processing (limit: {limit})...")
        
        # L·∫•y tin nh·∫Øn ch∆∞a x·ª≠ l√Ω
        result = self.get_unprocessed_messages(limit=limit)
        messages = result.get('messages', []) if isinstance(result, dict) else result
        
        if not messages:
            logger.info("No unprocessed messages found")
            return 0, 0
        
        processed_count = 0
        error_count = 0
        
        try:
            logger.info(f"üìã Processing {len(messages)} messages in batch mode")
            
            # T·∫°o prompt cho nhi·ªÅu messages
            batch_content = self.create_batch_prompt(messages)
            logger.info(f"üìù Batch prompt created with {len(messages)} messages")
            
            # G·ª≠i t·ªõi Groq ƒë·ªÉ b√≥c t√°ch th√¥ng tin cho t·∫•t c·∫£ messages
            logger.info("ü§ñ Processing batch with Groq...")
            groq_result = self.process_message_with_groq(batch_content)
            
            if groq_result:
                logger.info(f"‚úÖ Groq batch result received")
                
                # Parse JSON t·ª´ Groq response (expecting array)
                apartments_data = self.parse_groq_batch_response(groq_result)
                
                if apartments_data and len(apartments_data) > 0:
                    logger.info(f"üìä Parsed {len(apartments_data)} apartment(s) from batch")
                    logger.info(f"üìã Processing {len(messages)} message(s)")
                    
                    # ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng apartments kh·ªõp v·ªõi s·ªë l∆∞·ª£ng messages
                    if len(apartments_data) != len(messages):
                        logger.warning(f"‚ö†Ô∏è Mismatch: {len(apartments_data)} apartments but {len(messages)} messages")
                        logger.warning(f"‚ö†Ô∏è Will process {min(len(apartments_data), len(messages))} pairs")
                    
                    # X·ª≠ l√Ω t·ª´ng c·∫∑p apartment-message
                    num_to_process = min(len(apartments_data), len(messages))
                    for i in range(num_to_process):
                        try:
                            apartment_data = apartments_data[i]
                            message_id = messages[i]['id']
                            
                            logger.info(f"üè† Processing apartment {i+1}/{num_to_process} for message {message_id}")
                            
                            # Set data_status='REVIEWING' cho apartment data (gi·ªëng nh∆∞ API test)
                            apartment_data['data_status'] = 'REVIEWING'
                            
                            # Insert v√†o warehouse database
                            warehouse_result = self.insert_apartment_via_api(apartment_data)
                            
                            if warehouse_result:
                                logger.info(f"‚úÖ Warehouse insert/update successful for apartment {i+1}")
                                
                                # C·∫≠p nh·∫≠t warehouse_id cho t·∫•t c·∫£ messages c√πng content_hash
                                if isinstance(warehouse_result, int) and message_id:
                                    logger.info(f"üîÑ Attempting to update warehouse_id for message {message_id} to {warehouse_result}")
                                    
                                    update_success = self.update_message_warehouse_id(message_id, warehouse_result)
                                    
                                    if update_success:
                                        logger.info(f"‚úÖ Successfully updated warehouse_id for message {message_id}")
                                        processed_count += 1
                                    else:
                                        logger.error(f"‚ùå Failed to update warehouse_id for message {message_id}")
                                        error_count += 1
                                else:
                                    logger.warning(f"‚ö†Ô∏è Skipping warehouse_id update: warehouse_result={warehouse_result}, message_id={message_id}")
                                    error_count += 1
                            else:
                                logger.error(f"‚ùå Warehouse insert/update failed for apartment {i+1}")
                                error_count += 1
                                
                        except Exception as e:
                            logger.error(f"Error processing apartment {i+1} for message {messages[i].get('id', 'unknown')}: {e}")
                            error_count += 1
                else:
                    logger.error("‚ùå Failed to parse Groq batch response")
                    error_count = len(messages)  # T·∫•t c·∫£ messages ƒë·ªÅu l·ªói
            else:
                logger.error("‚ùå Failed to process batch with Groq")
                error_count = len(messages)  # T·∫•t c·∫£ messages ƒë·ªÅu l·ªói
                
        except Exception as e:
            logger.error(f"‚ùå Error in batch processing: {e}")
            error_count = len(messages)  # T·∫•t c·∫£ messages ƒë·ªÅu l·ªói
        
        logger.info(f"Batch processing completed. Processed: {processed_count}, Errors: {error_count}")
        return processed_count, error_count
    
    def run_test_mode(self, limit: int = 50):
        """Ch·∫ø ƒë·ªô test - ch·∫°y m·ªôt s·ªë tin nh·∫Øn ƒë·∫ßu ƒë·ªÉ th·ª≠"""
        logger.info(f"üß™ Running in TEST mode - processing {limit} messages")
        
        start_time = time.time()
        processed_count, error_count = self.process_messages_batch(limit=limit)
        elapsed_time = time.time() - start_time
        
        logger.info(f"‚úÖ TEST mode completed in {elapsed_time:.2f}s")
        logger.info(f"üìä Results: {processed_count} processed, {error_count} errors")
        
        return processed_count, error_count
    
    def get_message_by_id(self, message_id: int) -> Optional[Dict]:
        """
        L·∫•y tin nh·∫Øn theo ID
        
        Args:
            message_id: ID c·ªßa tin nh·∫Øn c·∫ßn l·∫•y
            
        Returns:
            Dict ch·ª©a th√¥ng tin tin nh·∫Øn ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            logger.info(f"üîç Fetching message with ID: {message_id}")
            
            with zalo_app.app_context():
                connection = self.get_zalo_db_connection()
                if not connection:
                    logger.error("‚ùå No database connection available")
                    return None
                
                from sqlalchemy import text
                
                query = text("""
                SELECT id, session_id, config_id, sender_id, sender_name, 
                       content, thread_id, thread_type, received_at, 
                       status_push_kafka, warehouse_id, reply_quote,
                       content_hash, added_document_chunks
                FROM zalo_received_messages 
                WHERE id = :message_id
                """)
                
                result = connection.execute(query, {"message_id": message_id})
                row = result.fetchone()
                
                if row:
                    message_data = dict(row._mapping)
                    logger.info(f"‚úÖ Found message {message_id}: {message_data.get('content', '')[:100]}...")
                    return message_data
                else:
                    logger.warning(f"‚ùå Message with ID {message_id} not found")
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Error fetching message {message_id}: {e}")
            return None
        finally:
            if connection:
                connection.close()
    
    def run_test_one_mode(self, message_id: int, real_insert: bool = False):
        """
        Ch·∫ø ƒë·ªô test one - test m·ªôt tin nh·∫Øn c·ª• th·ªÉ theo ID
        
        Args:
            message_id: ID c·ªßa tin nh·∫Øn c·∫ßn test
            real_insert: N·∫øu True, s·∫Ω th·ª±c s·ª± insert v√†o warehouse v√† c·∫≠p nh·∫≠t warehouse_id
        """
        mode_text = "REAL INSERT" if real_insert else "TEST MODE"
        logger.info(f"üß™ Running in TEST-ONE mode ({mode_text}) - processing message ID: {message_id}")
        
        start_time = time.time()
        
        try:
            # L·∫•y tin nh·∫Øn theo ID
            message = self.get_message_by_id(message_id)
            
            if not message:
                logger.error(f"‚ùå Message with ID {message_id} not found")
                return None, "Message not found"
            
            content = message['content']
            current_warehouse_id = message.get('warehouse_id')
            
            logger.info(f"üìù Message content: {content}")
            if current_warehouse_id:
                logger.info(f"üîÑ Message already has warehouse_id: {current_warehouse_id} - will replace")
            else:
                logger.info(f"üÜï Message has no warehouse_id - will create new")
            
            # G·ª≠i t·ªõi Groq ƒë·ªÉ b√≥c t√°ch th√¥ng tin
            logger.info("ü§ñ Processing with Groq...")
            groq_result = self.process_message_with_groq(content)
            
            
            if groq_result:
                logger.info(f"‚úÖ Groq result: {groq_result}")
                
                # Parse JSON t·ª´ Groq response
                apartment_data = self.parse_groq_response(groq_result)
                
                if apartment_data:
                    logger.info(f"üìä Parsed apartment data: {apartment_data}")
                    
                    # Insert/update v√†o warehouse database
                    if current_warehouse_id:
                        logger.info(f"üîÑ Replacing existing apartment (ID: {current_warehouse_id})...")
                    else:
                        logger.info("üè† Creating new apartment...")
                    
                    warehouse_result = self.insert_apartment_via_api(apartment_data)
                    
                    if warehouse_result:
                        logger.info("‚úÖ Warehouse insert/update successful")
                        
                        # Ch·ªâ c·∫≠p nh·∫≠t warehouse_id khi real_insert=True
                        if real_insert and isinstance(warehouse_result, int):
                            if current_warehouse_id:
                                logger.info(f"üîÑ Replacing warehouse_id from {current_warehouse_id} to {warehouse_result} for message {message_id}")
                            else:
                                logger.info(f"üÜï Setting warehouse_id {warehouse_result} for message {message_id}")
                            
                            update_success = self.update_message_warehouse_id(message_id, warehouse_result)
                            
                            if update_success:
                                logger.info(f"‚úÖ Successfully updated warehouse_id for message {message_id}")
                            else:
                                logger.error(f"‚ùå Failed to update warehouse_id for message {message_id}")
                        elif not real_insert:
                            logger.info("‚ÑπÔ∏è  Test mode - warehouse_id not updated to database")
                        
                        # Load full apartment data t·ª´ warehouse n·∫øu insert th√†nh c√¥ng
                        apartment_full_data = None
                        if isinstance(warehouse_result, int):
                            logger.info(f"üì• Loading full apartment data for ID: {warehouse_result}")
                            apartments_result = self.warehouse_service.get_apartments_by_ids([warehouse_result])
                            if apartments_result.get('success') and apartments_result.get('data'):
                                apartment_full_data = apartments_result['data'][0]
                                logger.info(f"‚úÖ Loaded full apartment data")
                        
                        result = {
                            'message_id': message_id,
                            'message_content': content,
                            'groq_result': groq_result,
                            'parsed_data': apartment_data,
                            'warehouse_success': True,
                            'apartment_id': warehouse_result if isinstance(warehouse_result, int) else None,
                            'apartment_full': apartment_full_data,  # Full data from warehouse
                            'real_insert': real_insert,
                            'replaced': current_warehouse_id is not None,
                            'previous_warehouse_id': current_warehouse_id
                        }
                    else:
                        logger.error("‚ùå Warehouse insert/update failed")
                        result = {
                            'message_id': message_id,
                            'message_content': content,
                            'groq_result': groq_result,
                            'parsed_data': apartment_data,
                            'warehouse_success': False,
                            'error': 'Warehouse insert/update failed'
                        }
                else:
                    logger.error("‚ùå Failed to parse Groq response")
                    result = {
                        'message_id': message_id,
                        'message_content': content,
                        'groq_result': groq_result,
                        'parsed_data': None,
                        'warehouse_success': False,
                        'error': 'Failed to parse Groq response'
                    }
            else:
                logger.error("‚ùå Failed to process message with Groq")
                result = {
                    'message_id': message_id,
                    'message_content': content,
                    'groq_result': None,
                    'parsed_data': None,
                    'warehouse_success': False,
                    'error': 'Failed to process with Groq'
                }
            
            elapsed_time = time.time() - start_time
            logger.info(f"‚úÖ TEST-ONE mode completed in {elapsed_time:.2f}s")
            
            return result, None
            
        except Exception as e:
            logger.error(f"‚ùå Error in test-one mode: {e}")
            return None, str(e)
    
    def run_batch_mode(self):
        """Ch·∫ø ƒë·ªô batch - ch·∫°y t·∫•t c·∫£ tin nh·∫Øn t·ª´ tr∆∞·ªõc ƒë·∫øn nay"""
        logger.info("üì¶ Running in BATCH mode - processing ALL unprocessed messages")
        
        total_processed = 0
        total_errors = 0
        batch_size = 100  # X·ª≠ l√Ω t·ª´ng batch 100 tin nh·∫Øn
        batch_count = 0
        
        start_time = time.time()
        
        while True:
            batch_count += 1
            logger.info(f"Processing batch {batch_count} (size: {batch_size})")
            
            processed_count, error_count = self.process_messages_batch(limit=batch_size)
            
            total_processed += processed_count
            total_errors += error_count
            
            # N·∫øu kh√¥ng c√≥ tin nh·∫Øn n√†o ƒë∆∞·ª£c x·ª≠ l√Ω, d·ª´ng l·∫°i
            if processed_count == 0:
                break
                
            logger.info(f"Batch {batch_count} completed: {processed_count} processed, {error_count} errors")
        
        elapsed_time = time.time() - start_time
        
        logger.info(f"‚úÖ BATCH mode completed in {elapsed_time:.2f}s")
        logger.info(f"üìä Total results: {total_processed} processed, {total_errors} errors across {batch_count} batches")
        
        return total_processed, total_errors
    
    def run_scheduler_mode(self):
        """Ch·∫ø ƒë·ªô scheduler - ch·∫°y ƒë·ªãnh k·ª≥ nh∆∞ hi·ªán t·∫°i"""
        logger.info(f"‚è∞ Running in SCHEDULER mode (interval: {self.interval//60} minutes)")
        
        while self.is_running:
            try:
                start_time = time.time()
                
                # X·ª≠ l√Ω batch tin nh·∫Øn
                self.process_messages_batch(limit=20)
                
                # T√≠nh th·ªùi gian c√≤n l·∫°i ƒë·ªÉ sleep
                elapsed_time = time.time() - start_time
                sleep_time = max(0, self.interval - elapsed_time)
                
                logger.info(f"Processing completed in {elapsed_time:.2f}s. Sleeping for {sleep_time/60:.1f} minutes")
                
                # Sleep v·ªõi ki·ªÉm tra is_running ƒë·ªÉ c√≥ th·ªÉ d·ª´ng nhanh
                for _ in range(int(sleep_time)):
                    if not self.is_running:
                        break
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"Error in scheduler: {e}")
                # Sleep 60 gi√¢y n·∫øu c√≥ l·ªói ƒë·ªÉ tr√°nh spam
                time.sleep(60)
    
    def run_scheduler(self):
        """Ch·∫°y scheduler ƒë·ªãnh k·ª≥ (legacy method)"""
        self.run_scheduler_mode()
    
    def start(self, interval_minutes: Optional[int] = None):
        """
        B·∫Øt ƒë·∫ßu service (c√≥ th·ªÉ start t·ª´ UI ngay c·∫£ khi schedule_enabled=False)
        
        Args:
            interval_minutes: Interval t√≠nh b·∫±ng ph√∫t (optional, n·∫øu kh√¥ng c√≥ th√¨ d√πng default_interval)
        """
        if self.is_running:
            logger.warning("Service is already running")
            return
        
        # Cho ph√©p set interval ƒë·ªông n·∫øu ƒë∆∞·ª£c truy·ªÅn v√†o
        if interval_minutes is not None:
            self.interval = interval_minutes * 60
            logger.info(f"Using custom interval: {interval_minutes} minutes")
        else:
            # N·∫øu kh√¥ng c√≥ interval t√πy ch·ªânh, d√πng default
            # N·∫øu default_interval = 0 (ZALO_MESSAGE_PROCESSOR_SCHEDULE=0), d√πng 10 ph√∫t l√†m m·∫∑c ƒë·ªãnh
            if self.default_interval > 0:
                self.interval = self.default_interval
            else:
                self.interval = 10 * 60  # 10 ph√∫t m·∫∑c ƒë·ªãnh khi ZALO_MESSAGE_PROCESSOR_SCHEDULE=0
                logger.info(f"Default interval is 0, using 10 minutes as fallback")
        
        # Validate interval
        if self.interval <= 0:
            logger.error("Cannot start schedule with interval <= 0")
            return
        
        self.is_running = True
        self.started_at = datetime.now().isoformat()
        self.thread = threading.Thread(target=self.run_scheduler, daemon=True)
        self.thread.start()
        
        logger.info(f"ZaloMessageProcessor service started (interval: {self.interval//60} minutes)")
    
    def stop(self):
        """D·ª´ng service"""
        if not self.is_running:
            logger.warning("Service is not running")
            return
        
        logger.info("üõë Stopping ZaloMessageProcessor service...")
        self.is_running = False
        
        if self.thread and self.thread.is_alive():
            logger.info("‚è≥ Waiting for thread to finish...")
            self.thread.join(timeout=10)  # TƒÉng timeout l√™n 10 gi√¢y
            
            if self.thread.is_alive():
                logger.warning("‚ö†Ô∏è Thread did not stop gracefully, forcing shutdown")
            else:
                logger.info("‚úÖ Thread stopped gracefully")
        
        logger.info("üõë ZaloMessageProcessor service stopped")
    
    def run_test_batch_mode(self, message_ids: List[int]):
        """
        Ch·∫ø ƒë·ªô test batch - x·ª≠ l√Ω nhi·ªÅu tin nh·∫Øn c√πng l√∫c trong m·ªôt prompt
        Lu√¥n insert v√†o warehouse v·ªõi data_status='REVIEWING' v√† c·∫≠p nh·∫≠t warehouse_id
        
        Args:
            message_ids: List ID c·ªßa c√°c tin nh·∫Øn c·∫ßn test
            
        Returns:
            Tuple (result_dict, error_message)
        """
        logger.info(f"üß™ Running in BATCH TEST mode - processing {len(message_ids)} messages")
        logger.info(f"üìã Message IDs: {message_ids}")
        
        start_time = time.time()
        
        try:
            # L·∫•y t·∫•t c·∫£ messages theo IDs
            messages = []
            for message_id in message_ids:
                message = self.get_message_by_id(message_id)
                if message:
                    messages.append(message)
                else:
                    logger.warning(f"‚ùå Message with ID {message_id} not found")
            
            if not messages:
                return None, "No valid messages found"
            
            logger.info(f"‚úÖ Found {len(messages)} valid messages out of {len(message_ids)} requested")
            
            # T·∫°o prompt cho nhi·ªÅu messages
            batch_content = self.create_batch_prompt(messages)
            logger.info(f"üìù Batch prompt created with {len(messages)} messages")
            
            # G·ª≠i t·ªõi Groq ƒë·ªÉ b√≥c t√°ch th√¥ng tin cho t·∫•t c·∫£ messages
            logger.info("ü§ñ Processing batch with Groq...")
            groq_result = self.process_message_with_groq(batch_content)
            print('groq_result', groq_result)
            
            if groq_result:
                logger.info(f"‚úÖ Groq batch result received")
                
                # Parse JSON t·ª´ Groq response (expecting array)
                apartments_data = self.parse_groq_batch_response(groq_result)
                
                
                if apartments_data and len(apartments_data) > 0:
                    logger.info(f"üìä Parsed {len(apartments_data)} apartment(s) from batch")
                    logger.info(f"üìã Processing {len(messages)} message(s)")
                    
                    # ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng apartments kh·ªõp v·ªõi s·ªë l∆∞·ª£ng messages
                    if len(apartments_data) != len(messages):
                        logger.warning(f"‚ö†Ô∏è Mismatch: {len(apartments_data)} apartments but {len(messages)} messages")
                        logger.warning(f"‚ö†Ô∏è Will process {min(len(apartments_data), len(messages))} pairs")
                    
                    # Insert/update v√†o warehouse database cho t·ª´ng apartment
                    results = []
                    warehouse_ids = []
                    
                    # X·ª≠ l√Ω t·ª´ng c·∫∑p apartment-message
                    num_to_process = min(len(apartments_data), len(messages))
                    for i in range(num_to_process):
                        apartment_data = apartments_data[i]
                        message_id = messages[i]['id']
                        
                        logger.info(f"üè† Processing apartment {i+1}/{num_to_process} for message {message_id}")
                        
                        # Set data_status='REVIEWING' cho apartment data
                        apartment_data['data_status'] = 'REVIEWING'
                        
                        # Insert v√†o warehouse database
                        warehouse_result = self.insert_apartment_via_api(apartment_data)
                        
                        apartment_result = {
                            'message_id': message_id,
                            'apartment_data': apartment_data,
                            'warehouse_success': False,
                            'apartment_id': None,
                            'replaced': False,
                            'previous_warehouse_id': None,
                            'price_rent': apartment_data.get('price_rent'),
                            'phone_number': apartment_data.get('phone_number')
                        }
                        
                        if warehouse_result:
                            logger.info(f"‚úÖ Warehouse insert/update successful for apartment {i+1}")
                            apartment_result['warehouse_success'] = True
                            apartment_result['apartment_id'] = warehouse_result if isinstance(warehouse_result, int) else None
                            
                            # Lu√¥n c·∫≠p nh·∫≠t warehouse_id sau khi insert th√†nh c√¥ng
                            if isinstance(warehouse_result, int) and message_id:
                                logger.info(f"üîÑ Attempting to update warehouse_id for message {message_id} to {warehouse_result}")
                                
                                # Ki·ªÉm tra xem message ƒë√£ c√≥ warehouse_id ch∆∞a
                                current_message = self.get_message_by_id(message_id)
                                current_warehouse_id = current_message.get('warehouse_id') if current_message else None
                                
                                if current_warehouse_id:
                                    logger.info(f"üîÑ Replacing warehouse_id from {current_warehouse_id} to {warehouse_result} for message {message_id}")
                                    apartment_result['replaced'] = True
                                    apartment_result['previous_warehouse_id'] = current_warehouse_id
                                else:
                                    logger.info(f"üÜï Setting warehouse_id {warehouse_result} for message {message_id}")
                                
                                update_success = self.update_message_warehouse_id(message_id, warehouse_result)
                                
                                if update_success:
                                    logger.info(f"‚úÖ Successfully updated warehouse_id for message {message_id}")
                                    warehouse_ids.append(warehouse_result)
                                else:
                                    logger.error(f"‚ùå Failed to update warehouse_id for message {message_id}")
                            else:
                                logger.warning(f"‚ö†Ô∏è Skipping warehouse_id update: warehouse_result={warehouse_result}, message_id={message_id}")
                        else:
                            logger.error(f"‚ùå Warehouse insert/update failed for apartment {i+1}")
                        
                        results.append(apartment_result)
                    
                    # Load full apartment data t·ª´ warehouse cho c√°c apartments ƒë√£ insert th√†nh c√¥ng
                    apartment_ids_to_load = [r['apartment_id'] for r in results if r.get('warehouse_success') and r.get('apartment_id')]
                    full_apartments_data = []
                    
                    if apartment_ids_to_load:
                        logger.info(f"üì• Loading full apartment data for {len(apartment_ids_to_load)} apartments")
                        apartments_result = self.warehouse_service.get_apartments_by_ids(apartment_ids_to_load)
                        if apartments_result.get('success'):
                            full_apartments_data = apartments_result.get('data', [])
                            logger.info(f"‚úÖ Loaded {len(full_apartments_data)} full apartment records")
                        else:
                            logger.warning(f"‚ö†Ô∏è Failed to load full apartment data: {apartments_result.get('error')}")
                    
                    elapsed_time = time.time() - start_time
                    
                    result = {
                        'batch_info': {
                            'message_ids': message_ids,
                            'processed_count': len(messages),
                            'apartment_count': len(apartments_data),
                            'successful_count': len([r for r in results if r['warehouse_success']]),
                            'processing_time': elapsed_time
                        },
                        'messages': messages,
                        'apartments': apartments_data,  # Raw data from Groq
                        'apartments_full': full_apartments_data,  # Full data from warehouse
                        'results': results,
                        'warehouse_ids': warehouse_ids,
                        'groq_result': groq_result
                    }
                    
                    logger.info(f"‚úÖ BATCH TEST mode completed in {elapsed_time:.2f}s")
                    logger.info(f"üìä Results: {len(messages)} messages, {len(apartments_data)} apartments, {len(warehouse_ids)} warehouse_ids updated")
                    
                    return result, None
                else:
                    logger.error("‚ùå Failed to parse Groq batch response")
                    return None, "Failed to parse Groq batch response"
            else:
                logger.error("‚ùå Failed to process batch with Groq")
                return None, "Failed to process batch with Groq"
                
        except Exception as e:
            logger.error(f"‚ùå Error in batch test mode: {e}")
            return None, str(e)
    
    def create_batch_prompt(self, messages: List[Dict]) -> str:
        """
        T·∫°o prompt cho nhi·ªÅu messages c√πng l√∫c
        
        Args:
            messages: List c√°c message dictionaries
            
        Returns:
            String prompt cho Groq
        """
        prompt_parts = [
            "C√°c tin nh·∫Øn c·∫ßn ph√¢n t√≠ch:",
            ""
        ]
        
        for i, message in enumerate(messages):
            prompt_parts.append(f"--- Tin nh·∫Øn {i+1} (ID: {message['id']}) ---")
            prompt_parts.append(message['content'])
            prompt_parts.append("")
        
        return "\n".join(prompt_parts)
    
    def parse_groq_batch_response(self, groq_response: str) -> List[Dict]:
        """
        Parse Groq response cho batch processing (expecting JSON array or single object)
        
        Args:
            groq_response: Response t·ª´ Groq
            
        Returns:
            List c√°c apartment dictionaries
        """
        try:
            response_clean = groq_response.strip()
            
            # Th·ª≠ t√¨m JSON array tr∆∞·ªõc [ { ... }, { ... } ]
            array_start = response_clean.find('[')
            if array_start != -1:
                # T√¨m closing bracket b·∫±ng c√°ch ƒë·∫øm balanced brackets
                bracket_count = 0
                array_end = -1
                for i in range(array_start, len(response_clean)):
                    if response_clean[i] == '[':
                        bracket_count += 1
                    elif response_clean[i] == ']':
                        bracket_count -= 1
                        if bracket_count == 0:
                            array_end = i
                            break
                
                if array_end != -1:
                    json_str = response_clean[array_start:array_end + 1]
                    try:
                        apartments = json.loads(json_str)
                        if isinstance(apartments, list):
                            logger.info(f"‚úÖ Parsed {len(apartments)} apartments from batch response (array format)")
                            return apartments
                    except json.JSONDecodeError as e:
                        logger.debug(f"Failed to parse as array: {e}")
            
            # N·∫øu kh√¥ng t√¨m th·∫•y array, th·ª≠ t√¨m single JSON object { ... }
            object_start = response_clean.find('{')
            if object_start != -1:
                # T√¨m closing brace b·∫±ng c√°ch ƒë·∫øm balanced braces
                brace_count = 0
                object_end = -1
                for i in range(object_start, len(response_clean)):
                    if response_clean[i] == '{':
                        brace_count += 1
                    elif response_clean[i] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            object_end = i
                            break
                
                if object_end != -1:
                    json_str = response_clean[object_start:object_end + 1]
                    try:
                        apartment = json.loads(json_str)
                        if isinstance(apartment, dict):
                            logger.info(f"‚úÖ Parsed 1 apartment from batch response (single object format, wrapped in array)")
                            return [apartment]  # Wrap single object in array
                    except json.JSONDecodeError as e:
                        logger.debug(f"Failed to parse as object: {e}")
            
            # N·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng th√†nh c√¥ng, log l·ªói chi ti·∫øt
            logger.error("‚ùå No valid JSON array or object found in Groq response")
            logger.error(f"Raw response preview: {response_clean[:500]}...")
            return []
                
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON decode error: {e}")
            logger.error(f"Raw response: {groq_response}")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error parsing Groq batch response: {e}")
            logger.error(f"Raw response preview: {groq_response[:500]}...")
            return []
    
    def get_status(self) -> Dict:
        """L·∫•y tr·∫°ng th√°i service"""
        return {
            'is_running': self.is_running,
            'thread_alive': self.thread.is_alive() if self.thread else False,
            'interval': self.interval,
            'interval_minutes': self.interval // 60,
            'schedule_enabled': self.schedule_enabled,  # Ch·ªâ l√† gi√° tr·ªã m·∫∑c ƒë·ªãnh t·ª´ env
            'started_at': getattr(self, 'started_at', None)
        }


# Global instance
zalo_processor = ZaloMessageProcessor()

def main():
    """Main function v·ªõi command line arguments"""
    parser = argparse.ArgumentParser(description='Zalo Message Processor Service')
    parser.add_argument('--mode', choices=['test', 'test-one', 'batch', 'scheduler'], default='scheduler',
                       help='Ch·∫ø ƒë·ªô ch·∫°y: test (50 tin nh·∫Øn), test-one (1 tin nh·∫Øn theo ID), batch (t·∫•t c·∫£), scheduler (ƒë·ªãnh k·ª≥)')
    parser.add_argument('--limit', type=int, default=50,
                       help='S·ªë l∆∞·ª£ng tin nh·∫Øn cho ch·∫ø ƒë·ªô test (default: 50)')
    parser.add_argument('--message-id', type=int,
                       help='ID c·ªßa tin nh·∫Øn c·∫ßn test (ch·ªâ d√πng v·ªõi mode test-one)')
    
    args = parser.parse_args()
    
    try:
        if args.mode == 'test':
            logger.info(f"üß™ Starting ZaloMessageProcessor in TEST mode (limit: {args.limit})")
            zalo_processor.run_test_mode(limit=args.limit)
            
        elif args.mode == 'test-one':
            if not args.message_id:
                logger.error("‚ùå --message-id is required for test-one mode")
                return
            logger.info(f"üß™ Starting ZaloMessageProcessor in TEST-ONE mode (message ID: {args.message_id})")
            result, error = zalo_processor.run_test_one_mode(args.message_id)
            
            if error:
                logger.error(f"‚ùå Test failed: {error}")
            else:
                logger.info("‚úÖ Test completed successfully")
                logger.info(f"üìä Result: {result}")
            
        elif args.mode == 'batch':
            logger.info("üì¶ Starting ZaloMessageProcessor in BATCH mode")
            zalo_processor.run_batch_mode()
            
        elif args.mode == 'scheduler':
            logger.info("‚è∞ Starting ZaloMessageProcessor in SCHEDULER mode")
            zalo_processor.start()
            
            # Gi·ªØ service ch·∫°y
            while zalo_processor.is_running:
                time.sleep(1)
                
    except KeyboardInterrupt:
        logger.info("Received interrupt signal, stopping service...")
        if args.mode == 'scheduler':
            zalo_processor.stop()
    except Exception as e:
        logger.error(f"Service error: {e}")
        if args.mode == 'scheduler':
            zalo_processor.stop()

if __name__ == "__main__":
    main()
